# Rule-of-thumb bandwidth for numerical derivative
# Adapted from https://github.com/mdcattaneo/replication-CJN_2020_ECMA.
# See Supplement, A.2.3.2.

#' Evaluate polynomial
#'
#' Evaluate the given polynomial at the given point. The function is vectorized
#' on its second argument.
#'
#' @param p The (ordered) coefficients of the polynomial, given as a vector
#'   whose first element is the constant term.
#' @param x The point at which the polynomial is evaluated.
#' @return The value of the polynomial
#' @keywords internal
#' @examples polyeval(c(1, -1, 2, -3), 0.5)
polyeval <- function(p, x) {
    s <- 0
    y <- 1
    for (f in p) {
        s <- s + f*y
        y <- y*x
    }
    return(s)
}

#' \loadmathjax
#' Common calculations for ROT log-likelihood
#'
#' @inheritParams rot
#' @param extBeta The extended vector of parameters \mjseqn{\tilde{\beta} =
#'   (1, \beta)}, of length \mjseqn{d + 1}.
#' @param gamma The vector of variance parameters \mjseqn{\gamma}, of length
#'   \mjseqn{k + 1}.
#' @return The optimal parameters.
#' @keywords internal
loglikelihoodCommon <- function(y, x, extBeta, gamma) {
    # The values z_i = {x^i}^T β.
    z <- as.vector(extBeta %*% x)
    # The values v_i = σ_u(x^i) = (\sum_{j=0}^k γ_j z_i^j)^(1/2).
    u <- polyeval(gamma, z)
    # The values of β and γ given can sometimes create a negative value for the
    # variance. Since we don't want to consider such cases, we just return
    # a large number, since we use a minimizing procedure to simulate
    # maximization.
    if (any(u < 0)) {
        return(1e3)
    }
    v <- sqrt(u)
    # The values w_i = Φ(z_i / v_i). Note that stats::pnorm is vectorized.
    # NOTE: Here w can equal 0 or 1. This will generate infinities in the log
    # function, which will in turn generate NaNs when multiplied by 0.
    w <- stats::pnorm(z / v)
    # The terms of the sum in the log-likelihood function.
    s <- y*log(w) + (1-y)*log(1-w)
    s[is.nan(s)] <- -Inf
    g <- mean(s)
    # Actually return the negative of the function value, in order to use a
    # minimizing procedure to compute its argmax.
    return(min(-g, 1e3))
}

#' \loadmathjax
#' ROT log-likelihood with variable beta
#'
#' @inheritParams rot
#' @param par The concatenated vector of parameters.
#' @return The optimal parameters.
#' @keywords internal
loglikelihoodVarBeta <- function(y, x, par) {
    d <- dim(x)[1] - 1
    n <- dim(x)[2]
    k <- length(par) - (d + 1)
    extBeta <- c(1, par[1:d])
    gamma <- par[(d+1):(d+k+1)]
    return(loglikelihoodCommon(y, x, extBeta, gamma))
}

#' \loadmathjax
#' ROT log-likelihood with fixed beta
#'
#' @inheritParams rot
#' @param betaEst The vector of (free) parameters \mjseqn{\beta}.
#' @param par The vector of parameters \mjseqn{\gamma}.
#' @return The optimal parameters.
#' @keywords internal
loglikelihoodFixedBeta <- function(y, x, betaEst, par) {
    extBeta <- c(1, betaEst)
    gamma <- par
    return(loglikelihoodCommon(y, x, extBeta, gamma))
}

# TODO _1 -> _f
# TODO note the discrepancy in the F0_3_1 calculations!
# maybe write up the partial derivative analysis in a separate pdf

#' \loadmathjax
#' ROT calculation of bandwidth
#'
#' Rule-Of-Thumb (ROT) calculation of step/bandwidth parameters, used in the
#' calculation of the estimate \mjseqn{\tilde{H}_n}. For more details, see the
#' paper by Cattaneo et al. (2020), linked in the documentation of
#' \code{\link{newBootstrapCR}}, and its supplement, specifically section
#' A.2.3.\cr
#' Adapted from the [authors' implementation](https://github.com/mdcattaneo/replication-CJN_2020_ECMA).
#'
#' This procedure has three steps. In the first, we assume that the data are
#' generated by a (finitely) parametrized model, and we estimate these
#' parameters using maximum likelihood estimation. We then use these parameters
#' to calculate estimates for certain bias and variance constants. Finally,
#' these constants are used in the calculation of the step or bandwidth
#' parameters.
#'
#' # Model
#' We assume, as per the paper, that the data are generated by the model
#' \mjsdeqn{
#'      y = \mathbb{1}\left\lbrace \tilde{\beta}^T x + u \geq 0 \right\rbrace},
#' where \mjseqn{\mathbb{1}\left\lbrace\cdot\right\rbrace} is the indicator
#' function/Iverson bracket,
#' \mjseqn{x} is a vector of regressors of length \mjseqn{d+1},
#' \mjseqn{\tilde{\beta} = (1, \beta)} is the extended vector of (true)
#' parameters (which, in the paper, is called \mjseqn{\beta_0}),
#' \mjseqn{y} is the observed variable, which in our case is always equal to
#' \mjseqn{1}, and \mjseqn{u} is a random variable whose distribution,
#' conditioned on \mjseqn{x}, is normal with mean \mjseqn{0} and variance
#' \mjseqn{s_u(x) = \sigma_u^2(x) = \gamma^T p(x)}.
#'
#' According to the supplement, \mjseqn{p(x)} is "a polynomial expansion", but
#' from inspecting the
#' [github code](https://github.com/mdcattaneo/replication-CJN_2020_ECMA/blob/2c1bbea2190936c0697540833b93953a012bf618/main_function_maxscore.R#L72),
#' it seems that it actually is an expansion of the scalar
#' \mjseqn{z = \tilde{\beta}^T x}, i.e. \mjseqn{p(x) = (z^0, z^1, \dots, z^k)}.
#'
#' Let us partition \mjseqn{x = (x_f, x_r)}, where the two parts have length
#' \mjseqn{1} (a scalar) and \mjseqn{d} respectively. The authors further
#' assume that the distibution of \mjseqn{x_f} conditional to \mjseqn{x_r} is
#' normal with mean \mjseqn{\mu_1} and variance \mjseqn{\sigma_1^2}.
#'
#' The model is now parametrized by \mjseqn{\beta} and
#' \mjseqn{\gamma = (\gamma_0, \gamma_1, \dots, \gamma_k)}.
#' This is basically a heteroskedastic probit model, since the conditional cdf
#' is
#' \mjsdeqn{
#'      F_{u | x}(u | x) = F_{u | x_f, x_r}(u | x_f, x_r) =
#'      \Phi(\frac{u}{\sigma_u(x)})},
#' where \mjseqn{\Phi(z)} is the cdf of the standard normal distribution.
#'
#' Consider now that we have \mjseqn{n} pairs of observations \mjseqn{(y^i,
#' x^i)}. Again, note that in our case, \mjseqn{y^i = 1} always. Then the
#' probability of \mjseqn{y^i = 1} given \mjseqn{x = x^i} is
#' \mjsdeqn{
#'      \pi_i =
#'      \mathbb{P}(y = 1 | x = x^i) =
#'      \mathbb{P}(\tilde{\beta}^T x + u \geq 0 | x = x^i) =
#'      F_{u | x}(\tilde{\beta}^T x^i) =
#'      \Phi(\frac{u}{\sigma_u(x)})},
#' because of the symmetry of the normal cdf.
#' Likewise, the probability of \mjseqn{y^i = 0} given \mjseqn{x = x^i} is
#' \mjsdeqn{1 - \pi_i = 1 - F_{u | x}(\tilde{\beta}^T x^i)}.
#'
#' The log-likelihood function can be written as
#' \mjsdeqn{
#'      L(\beta, \gamma; Y, X) =
#'      \sum_{i=1}^n ( y_i \log(\pi_i) + (1-y_i) \log(1-\pi_i) )}.
#' We can then estimate the parameters \mjseqn{\beta} and \mjseqn{\gamma} using
#' maximum likelihood estimation. However, in the usual case, we have already
#' calculated an estimate \mjseqn{\hat{\beta}} using the score optimization
#' procedure. We can therefore define a related log-likelihood function:
#' \mjsdeqn{\tilde{L}(\gamma; Y, X) = L(\hat{\beta}, \gamma; Y, X)}
#' and estimate only \mjseqn{\gamma}. These two functions are called
#' \code{\link{loglikelihoodVarBeta}} and \code{\link{loglikelihoodFixedBeta}}
#' respectively.
#'
#' Note that in the first case, the vector `par` of parameters, used in the
#' optimization procedure for MLE, has length \mjseqn{d+k+1}: its first
#' \mjseqn{d} elements correspond to the elements of \mjseqn{\beta}, and the
#' rest to the elements of \mjseqn{\gamma}. If estimating only \mjseqn{\gamma},
#' then `par` has length \mjseqn{k+1}. Initial values for \mjseqn{\beta} (if
#' used) is the ones-vector, and for \mjseqn{\gamma} is the vector
#' \mjseqn{(1, 0, \dots, 0)}.
#'
#' Note that, for the estimation of \mjseqn{\mu_1} and \mjseqn{\sigma_1}, we
#' use the sample mean and standard deviation.
#'
#' ## TODO subsection name 1
#' The next step is to calculate certain values of the family of functions
#' \mjseqn{F_0^{i,j}(x_r)}, defined on pg. 15 of the supplement. Specifically:
#' \mjsdeqn{
#'      F_0^{i,j}(x_r) =
#'      \frac{\partial^i}{\partial u^i} F_{u|x_f,x_r}(-u|x_f+u,x_r)
#'      \frac{\partial^j}{\partial x_f^j} F_{x_f|x_r}(x_f|x_r)
#'      \Biggr\rvert_{u=0,x_f=-\beta^T x_r}}
#' , where \mjseqn{F_{u|x_f,x_r}} and \mjseqn{F_{x_f|x_r}} are the
#' corresponding conditional cdfs.
#'
#' Evaluating the partial derivatives for \mjseqn{(i,j) = (0,1), (1,3), (3,1)},
#' we can prove that:
#' \mjsdeqn{F_0^{0,1}(x_r) = .}
#' \mjsdeqn{F_0^{1,3}(x_r) = .}
#' \mjsdeqn{F_0^{3,1}(x_r) = .}
#'
#' # Bias and variance constants
#' TODO
#'
#' # Step/bandwidth parameters
#' TODO
#'
#' @param y The vector of observations, of length \mjseqn{n}. This should
#'   always be equal to all ones.
#' @param x The array of regressors, also called the *data array* in other
#'   functions. An array of size \mjseqn{(d+1) \times n}, where \mjseqn{d} is
#'   the number of free attributes, and also the length of the vector
#'   \mjseqn{\beta}.
#' @param k The length of the vector of parameters \mjseqn{\gamma}, reduced by
#'   \mjseqn{1}. Should be at least \mjseqn{2}.
#' @param betaEst The vector \mjseqn{\hat{\beta}} of optimal parameters. If
#'   omitted, then both \mjseqn{\beta} and \mjseqn{\gamma} are jointly
#'   estimated.
#' @param debugLogging Whether this function should print information for
#'   debugging purposes.
#' @return TODO
#' @keywords internal
rot <- function(y, x, k, betaEst = NULL, debugLogging = FALSE) {
    stopifnot(k >= 2)
    d <- dim(x)[1] - 1
    n <- dim(x)[2]
    if (is.null(betaEst)) {
        beta0 <- rep(1, d)
        gamma0 <- c(1, rep(0, k))
        par0 <- c(beta0, gamma0)
        optimArgs <- list(par0, loglikelihoodVarBeta,   y = y, x = x)
    } else {
        gamma0 <- c(1, rep(0, k))
        par0 <- gamma0
        optimArgs <- list(par0, loglikelihoodFixedBeta, y = y, x = x, betaEst = betaEst)
    }
    optResult <- do.call(stats::optim, optimArgs)
    optPars <- optResult$par
    if (debugLogging) {
        print("[DEBUG] in rot: optPars =")
        print(optPars)
    }
    betaR <- if (is.null(betaEst)) { optPars[1:d] } else { betaEst }
    gamma <- if (is.null(betaEst)) { optPars[(d+1):(d+k+1)] } else { optPars }
    # For μ_1 and σ_1, we use the sample mean and std.
    mu1 <- mean(x[1, ])
    sigma1 <- stats::sd(x[1, ])
    # The vector of values (β_r^T (x_r^i)^T + μ_1)/σ_1, for i = 1,...,n, i.e.
    # the argument of φ(.) in the formulas at the top of pg. 20 of the
    # supplement.
    p <- (as.vector(betaR %*% x[2:(d+1), ]) + mu1) / sigma1
    # Note that, according to our definitions,
    #   s_u(x_1, x_r)   @ {x_1 = -x_r^T β_r} = γ_0
    #   s'_u(x_1, x_r)  @ {x_1 = -x_r^T β_r} = γ_1
    #   s''_u(x_1, x_r) @ {x_1 = -x_r^T β_r} = 2 γ_2
    # where s'_u(x) = ∂/∂x_1 s_u(x), s''_u(x) = ∂^2/∂x_1^2 s_u(x).
    # However, in the supplement, on pg. 20, Cattaneo et al. have σ_u(x) and
    # σ_u^3(x), where they have defined σ_u^2(x) = s_u(x), while in their
    # github code they use σ_u^2(x) for σ_u(x).
    # After communicating with the authors, it turns out that this is indeed an
    # inconsistency. We recover the correct values of
    # q_j = ∂^j/∂x_1^j σ_u(x) @ {x_1 = -x_r^T β_r}, which are:
    #   * q_0 = γ_0^(1/2)
    #   * q_1 = 1/2 γ_0^(-1/2) γ_1
    #   * q_2 = -1/4 γ_0^(-3/2) γ_1^2 + γ_0^(-1/2) γ_2
    # I still haven't verified these with Mathematica, but I have triple-checked
    # my calculations.
    # Note that F_0^{0,1}(x_r) can be easily shown to be equal to
    #   (1/(2*σ_1)) φ((x_r^T β_r + μ_1)/σ_1),
    # where φ is the pdf of the standard normal distribution.
    gamma_0 <- gamma[1]
    gamma_1 <- gamma[2]
    gamma_2 <- gamma[3]
    q_0 <- gamma_0^(1/2)
    q_1 <- (1/2)*gamma_0^(-1/2)*gamma_1
    q_2 <- -(1/4)*gamma_0^(-3/2)*gamma_1^2 + gamma_0^(-1/2)*gamma_2
    # q_0 <- sqrt(gamma_0)
    # q_1 <- sqrt(gamma_1)
    # q_2 <- sqrt(gamma_2)
    F0_1_3 <- -(stats::dnorm(0) / (q_0 * sigma1^3)) * stats::dnorm(p) * (p^2 - 1)
    F0_3_1 <-  (stats::dnorm(0) / (q_0^3 * sigma1)) * stats::dnorm(p) * (1 - q_2*q_0 + 2*q_1^2)
    F0_0_1 <- (1 / (2*sigma1)) * stats::dnorm(p)
    # See the makeHmatrix function in confidence.R.
    # NOTE:
    # The calculations for B.ker and V.ker rely on the values of two definite
    # integrals involving the first derivative of the kernel function.
    # The only kernel function currently available is K(u) = φ(u) (the pdf of
    # the standard normal distribution), whose first derivative is
    # K'(u) = -u*φ(u). The definite integrals we need are:
    # * the integral over the reals of u -> u^3*K'(u), and
    # * the integral over the reals of u -> K'(u)^2.
    # In our case, these can be proven to be equal to -3 and 1/(4√π)
    # respectively. These values will have to be changed if other kernels are
    # used.
    integral1 <- -3
    integral2 <- 1/(4*sqrt(pi))
    B.nd  <- matrix(0, d, d)
    B.ker <- matrix(0, d, d)
    V.nd  <- matrix(0, d, d)
    V.ker <- matrix(0, d, d)
    for (col in 1:d) {
        for (row in 1:d) {
            x_row <- as.vector(x[row + 1, ])
            x_col <- as.vector(x[col + 1, ])
            B.nd[row, col]  = -mean((F0_1_3 + F0_3_1/3)*(x_row*x_col)*(x_row^2 + x_col^2))
            B.ker[row, col] = integral1*mean((F0_1_3 + F0_3_1/3)*(x_row*x_col))
            f <- 2*abs(x_row) + 2*abs(x_col) - abs(x_row + x_col) - abs(x_row - x_col)
            V.nd[row, col]  <- (1/8)*mean(f * F0_0_1)
            V.ker[row, col] <- 2*integral2*mean(F0_0_1*x_row^2*x_col^2)
        }
    }
    if (debugLogging) {
        print("[DEBUG] in rot: B.nd =")
        print(B.nd)
        print("[DEBUG] in rot: V.nd =")
        print(V.nd)
        print("[DEBUG] in rot: B.ker =")
        print(B.ker)
        print("[DEBUG] in rot: V.ker =")
        print(V.ker)
    }
    V.nd.sum <- sum(V.nd)
    B.nd.sqsum <- sum(B.nd^2)
    # r_n, in the formulas (the effective sample size?) seems to equal to n^(1/3).
    nd.h  <- (3*V.nd/4/B.nd^2)^(1/7)*n^(-1/7)
    ker.h <- (3*V.ker/4/B.ker^2)^(1/7)*n^(-1/7)
    nd.h.summed <- (3*V.nd.sum/4/B.nd.sqsum)^(1/7)*n^(-1/7)
    return(list(bw.nd = nd.h, bw.ker = ker.h, bw.nd.summed = nd.h.summed))
}
