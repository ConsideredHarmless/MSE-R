% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rot.R
\name{rot}
\alias{rot}
\title{\loadmathjax
ROT calculation of bandwidth}
\usage{
rot(y, x, k, betaEst = NULL, debugLogging = FALSE)
}
\arguments{
\item{y}{The vector of observations, of length \mjseqn{n}. This should
always be equal to all ones.}

\item{x}{The array of regressors, also called the \emph{data array} in other
functions. An array of size \mjseqn{(d+1) \times n}, where \mjseqn{d} is
the number of free attributes, and also the length of the vector
\mjseqn{\beta}.}

\item{k}{The length of the vector of parameters \mjseqn{\gamma}, reduced by
\mjseqn{1}. Should be at least \mjseqn{2}.}

\item{betaEst}{The vector \mjseqn{\hat{\beta}} of optimal parameters. If
omitted, then both \mjseqn{\beta} and \mjseqn{\gamma} are jointly
estimated.}

\item{debugLogging}{Whether this function should print information for
debugging purposes.}
}
\value{
TODO
}
\description{
Rule-Of-Thumb (ROT) calculation of step/bandwidth parameters, used in the
calculation of the estimate \mjseqn{\tilde{H}_n}. For more details, see the
paper by Cattaneo et al. (2020), linked in the documentation of
\code{\link{newBootstrapCR}}, and its supplement, specifically section
A.2.3.\cr
Adapted from the \href{https://github.com/mdcattaneo/replication-CJN_2020_ECMA}{authors' implementation}.
}
\details{
This procedure has three steps. In the first, we assume that the data are
generated by a (finitely) parametrized model, and we estimate these
parameters using maximum likelihood estimation. We then use these parameters
to calculate estimates for certain bias and variance constants. Finally,
these constants are used in the calculation of the step or bandwidth
parameters.
}
\section{Model}{
We assume, as per the paper, that the data are generated by the model
\mjsdeqn{
     y = \mathbb{1}\left\lbrace \tilde{\beta}^T x + u \geq 0 \right\rbrace},
where \mjseqn{\mathbb{1}\left\lbrace\cdot\right\rbrace} is the indicator
function/Iverson bracket,
\mjseqn{x} is a vector of regressors of length \mjseqn{d+1},
\mjseqn{\tilde{\beta} = (1, \beta)} is the extended vector of (true)
parameters (which, in the paper, is called \mjseqn{\beta_0}),
\mjseqn{y} is the observed variable, which in our case is always equal to
\mjseqn{1}, and \mjseqn{u} is a random variable whose distribution,
conditioned on \mjseqn{x}, is normal with mean \mjseqn{0} and variance
\mjseqn{s_u(x) = \sigma_u^2(x) = \gamma^T p(x)}.

According to the supplement, \mjseqn{p(x)} is "a polynomial expansion", but
from inspecting the
\href{https://github.com/mdcattaneo/replication-CJN_2020_ECMA/blob/2c1bbea2190936c0697540833b93953a012bf618/main_function_maxscore.R#L72}{github code},
it seems that it actually is an expansion of the scalar
\mjseqn{z = \tilde{\beta}^T x}, i.e. \mjseqn{p(x) = (z^0, z^1, \dots, z^k)}.

Let us partition \mjseqn{x = (x_f, x_r)}, where the two parts have length
\mjseqn{1} (a scalar) and \mjseqn{d} respectively. The authors further
assume that the distibution of \mjseqn{x_f} conditional to \mjseqn{x_r} is
normal with mean \mjseqn{\mu_1} and variance \mjseqn{\sigma_1^2}.

The model is now parametrized by \mjseqn{\beta} and
\mjseqn{\gamma = (\gamma_0, \gamma_1, \dots, \gamma_k)}.
This is basically a heteroskedastic probit model, since the conditional cdf
is
\mjsdeqn{
     F_{u | x}(u | x) = F_{u | x_f, x_r}(u | x_f, x_r) =
     \Phi(\frac{u}{\sigma_u(x)})},
where \mjseqn{\Phi(z)} is the cdf of the standard normal distribution.

Consider now that we have \mjseqn{n} pairs of observations \mjseqn{(y^i,
x^i)}. Again, note that in our case, \mjseqn{y^i = 1} always. Then the
probability of \mjseqn{y^i = 1} given \mjseqn{x = x^i} is
\mjsdeqn{
     \pi_i =
     \mathbb{P}(y = 1 | x = x^i) =
     \mathbb{P}(\tilde{\beta}^T x + u \geq 0 | x = x^i) =
     F_{u | x}(\tilde{\beta}^T x^i) =
     \Phi(\frac{u}{\sigma_u(x)})},
because of the symmetry of the normal cdf.
Likewise, the probability of \mjseqn{y^i = 0} given \mjseqn{x = x^i} is
\mjsdeqn{1 - \pi_i = 1 - F_{u | x}(\tilde{\beta}^T x^i)}.

The log-likelihood function can be written as
\mjsdeqn{
     L(\beta, \gamma; Y, X) =
     \sum_{i=1}^n ( y_i \log(\pi_i) + (1-y_i) \log(1-\pi_i) )}.
We can then estimate the parameters \mjseqn{\beta} and \mjseqn{\gamma} using
maximum likelihood estimation. However, in the usual case, we have already
calculated an estimate \mjseqn{\hat{\beta}} using the score optimization
procedure. We can therefore define a related log-likelihood function:
\mjsdeqn{\tilde{L}(\gamma; Y, X) = L(\hat{\beta}, \gamma; Y, X)}
and estimate only \mjseqn{\gamma}. These two functions are called
\code{\link{loglikelihoodVarBeta}} and \code{\link{loglikelihoodFixedBeta}}
respectively.

Note that in the first case, the vector \code{par} of parameters, used in the
optimization procedure for MLE, has length \mjseqn{d+k+1}: its first
\mjseqn{d} elements correspond to the elements of \mjseqn{\beta}, and the
rest to the elements of \mjseqn{\gamma}. If estimating only \mjseqn{\gamma},
then \code{par} has length \mjseqn{k+1}. Initial values for \mjseqn{\beta} (if
used) is the ones-vector, and for \mjseqn{\gamma} is the vector
\mjseqn{(1, 0, \dots, 0)}.

Note that, for the estimation of \mjseqn{\mu_1} and \mjseqn{\sigma_1}, we
use the sample mean and standard deviation.
\subsection{TODO subsection name 1}{

The next step is to calculate certain values of the family of functions
\mjseqn{F_0^{i,j}(x_r)}, defined on pg. 15 of the supplement. Specifically:
\mjsdeqn{
     F_0^{i,j}(x_r) =
     \frac{\partial^i}{\partial u^i} F_{u|x_f,x_r}(-u|x_f+u,x_r)
     \frac{\partial^j}{\partial x_f^j} F_{x_f|x_r}(x_f|x_r)
     \Biggr\rvert_{u=0,x_f=-\beta^T x_r}}
, where \mjseqn{F_{u|x_f,x_r}} and \mjseqn{F_{x_f|x_r}} are the
corresponding conditional cdfs.

Evaluating the partial derivatives for \mjseqn{(i,j) = (0,1), (1,3), (3,1)},
we can prove that:
\mjsdeqn{F_0^{0,1}(x_r) = .}
\mjsdeqn{F_0^{1,3}(x_r) = .}
\mjsdeqn{F_0^{3,1}(x_r) = .}
}
}

\section{Bias and variance constants}{
TODO
}

\section{Step/bandwidth parameters}{
TODO
}

\keyword{internal}
