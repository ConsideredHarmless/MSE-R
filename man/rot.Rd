% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rot.R
\name{rot}
\alias{rot}
\title{\loadmathjax
ROT calculation of bandwidth}
\usage{
rot(y, x, k, betaEst = NULL, debugLogging = FALSE)
}
\arguments{
\item{y}{The vector of observations, of length \mjseqn{n}. This should
always be equal to all ones.}

\item{x}{The array of regressors, also called the \emph{data array} in other
functions. An array of size \mjseqn{(d+1) \times n}, where \mjseqn{d} is
the number of free attributes, and also the length of the vector
\mjseqn{\beta}.}

\item{k}{The length of the vector of parameters \mjseqn{\gamma}, reduced by
\mjseqn{1}. Should be at least \mjseqn{2}.}

\item{betaEst}{The vector \mjseqn{\hat{\beta}} of optimal parameters. If
omitted, then both \mjseqn{\beta} and \mjseqn{\gamma} are jointly
estimated.}

\item{debugLogging}{Whether this function should print information for
debugging purposes.}
}
\value{
A list with members:
\tabular{ll}{
\code{$nd} \tab The matrix-valued step parameter. \cr
\code{$nd.summer} \tab The scalar-valued step parameter. \cr
\code{$ker} \tab The matrix-valued bandwidth.
}
}
\description{
Rule-Of-Thumb (ROT) calculation of step/bandwidth parameters, used in the
calculation of the estimate \mjseqn{\tilde{H}_n}. For more details, see the
paper by Cattaneo et al. (2020), linked in the documentation of
\code{\link{newBootstrapCR}}, and its supplement, specifically section
A.2.3.\cr
Adapted from the \href{https://github.com/mdcattaneo/replication-CJN_2020_ECMA}{authors' implementation}.
}
\details{
This procedure has three steps. In the first, we assume that the data are
generated by a (finitely) parametrized model, and we estimate these
parameters using maximum likelihood estimation. We then use these parameters
to calculate estimates for certain bias and variance constants. Finally,
these constants are used in the calculation of the step or bandwidth
parameters.
}
\section{Model}{
We assume, as per the paper, that the data are generated by the model
\mjsdeqn{
     y = \mathbb{1}\left\lbrace \tilde{\beta}^T x + u \geq 0 \right\rbrace},
where \mjseqn{\mathbb{1}\left\lbrace\cdot\right\rbrace} is the indicator
function/Iverson bracket,
\mjseqn{x} is a vector of regressors of length \mjseqn{d+1},
\mjseqn{\tilde{\beta} = (1, \beta)} is the extended vector of (true)
parameters (which, in the paper, is called \mjseqn{\beta_0}),
\mjseqn{y} is the observed variable, which in our case is always equal to
\mjseqn{1}, and \mjseqn{u} is a random variable whose distribution,
conditioned on \mjseqn{x}, is normal with mean \mjseqn{0} and variance
\mjseqn{s_u(x) = \sigma_u^2(x) = \gamma^T p(x)}.

According to the supplement, \mjseqn{p(x)} is "a polynomial expansion", but
from inspecting the
\href{https://github.com/mdcattaneo/replication-CJN_2020_ECMA/blob/2c1bbea2190936c0697540833b93953a012bf618/main_function_maxscore.R#L72}{github code},
it seems that it actually is an expansion of the scalar
\mjseqn{z = \tilde{\beta}^T x}, i.e. \mjseqn{p(x) = (z^0, z^1, \dots, z^k)}.

Let us partition \mjseqn{x = (x_f, x_r)}, where the two parts have length
\mjseqn{1} (a scalar) and \mjseqn{d} respectively. The authors further
assume that the distibution of \mjseqn{x_f} conditional to \mjseqn{x_r} is
normal with mean \mjseqn{\mu_1} and variance \mjseqn{\sigma_1^2}.

The model is now parametrized by \mjseqn{\beta} and
\mjseqn{\gamma = (\gamma_0, \gamma_1, \dots, \gamma_k)}.
This is basically a heteroskedastic probit model, since the conditional cdf
is
\mjsdeqn{
     F_{u | x}(u | x) = F_{u | x_f, x_r}(u | x_f, x_r) =
     \Phi(\frac{u}{\sigma_u(x)})},
where \mjseqn{\Phi(z)} is the cdf of the standard normal distribution.

Consider now that we have \mjseqn{n} pairs of observations \mjseqn{(y^i,
x^i)}. Again, note that in our case, \mjseqn{y^i = 1} always. Then the
probability of \mjseqn{y^i = 1} given \mjseqn{x = x^i} is
\mjsdeqn{
     \pi_i =
     \mathbb{P}(y = 1 | x = x^i) =
     \mathbb{P}(\tilde{\beta}^T x + u \geq 0 | x = x^i) =
     F_{u | x}(\tilde{\beta}^T x^i) =
     \Phi(\frac{u}{\sigma_u(x)})},
because of the symmetry of the normal cdf.
Likewise, the probability of \mjseqn{y^i = 0} given \mjseqn{x = x^i} is
\mjsdeqn{1 - \pi_i = 1 - F_{u | x}(\tilde{\beta}^T x^i)}.

The log-likelihood function can be written as
\mjsdeqn{
     L(\beta, \gamma; Y, X) =
     \sum_{i=1}^n ( y_i \log(\pi_i) + (1-y_i) \log(1-\pi_i) )}.
We can then estimate the parameters \mjseqn{\beta} and \mjseqn{\gamma} using
maximum likelihood estimation. However, in the usual case, we have already
calculated an estimate \mjseqn{\hat{\beta}} using the score optimization
procedure. We can therefore define a related log-likelihood function:
\mjsdeqn{\tilde{L}(\gamma; Y, X) = L(\hat{\beta}, \gamma; Y, X)}
and estimate only \mjseqn{\gamma}. These two functions are called
\code{\link{loglikelihoodVarBeta}} and \code{\link{loglikelihoodFixedBeta}}
respectively.

Note that in the first case, the vector \code{par} of parameters, used in the
optimization procedure for MLE, has length \mjseqn{d+k+1}: its first
\mjseqn{d} elements correspond to the elements of \mjseqn{\beta}, and the
rest to the elements of \mjseqn{\gamma}. If estimating only \mjseqn{\gamma},
then \code{par} has length \mjseqn{k+1}. Initial values for \mjseqn{\beta} (if
used) is the ones-vector, and for \mjseqn{\gamma} is the vector
\mjseqn{(1, 0, \dots, 0)}.

Note that, for the estimation of \mjseqn{\mu_1} and \mjseqn{\sigma_1}, we
use the sample mean and standard deviation.
\subsection{Values of \mjseqn{F_0}}{

The next step is to calculate certain values of the family of functions
\mjseqn{F_0^{i,j}(x_r)}, defined on pg. 15 of the supplement. Specifically:
\mjsdeqn{
     F_0^{i,j}(x_r) =
     \frac{\partial^i}{\partial u^i} F_{u|x_f,x_r}(-u|x_f+u,x_r)
     \frac{\partial^j}{\partial x_f^j} F_{x_f|x_r}(x_f|x_r)
     \Biggr\rvert_{u=0,x_f=-\beta^T x_r}}
, where \mjseqn{F_{u|x_f,x_r}} and \mjseqn{F_{x_f|x_r}} are the
corresponding conditional cdfs.

Cattaneo et al. provide the following formulas for the partial derivatives
for \mjseqn{(i,j) = (0,1), (1,3), (3,1)}:
\mjsdeqn{F_0^{0,1}(x_r) = \frac{1}{2} \sigma_f^{-1} \phi(w)}
\mjsdeqn{F_0^{1,3}(x_r) = \phi(0) \phi(w) (1 - w^2) \sigma_f^{-3} \tau^{-1}}
\mjsdeqn{F_0^{3,1}(x_r) = \phi(0) \phi(w) \sigma_f^{-1} \tau^{-3}
    (1 - \tau \ddot\tau + 2 {\dot\tau}^2)}
where
\mjsdeqn{w = \frac{\beta^T x_r + \mu_f}{\sigma_f}}
\mjsdeqn{\tau = \sigma_u(x_f, x_r) \biggr\rvert_{x_f=-\beta^T x_r} }
\mjsdeqn{\dot\tau = \frac{\partial}{\partial x_f} \sigma_u(x_f, x_r) \biggr\rvert_{x_f=-\beta^T x_r} }
\mjsdeqn{\ddot\tau = \frac{\partial^2}{\partial x_f^2} \sigma_u(x_f, x_r) \biggr\rvert_{x_f=-\beta^T x_r} }

If using a polynomial expansion for \mjseqn{s_u}, then the values for
\mjseqn{\tau, \dot\tau, \ddot\tau} are
\mjseqn{\gamma_0^{\frac{1}{2}}, \frac{1}{2}\gamma_0^{-\frac{1}{2}}\gamma_1,
    -\frac{1}{4}\gamma_0^{-\frac{3}{2}}\gamma_1^2 + \gamma_0^{-\frac{1}{2}}\gamma_2},
respectively. Alternatively, if the polynomial expansion is for
\mjseqn{\sigma_u} instead, they are \mjseqn{\gamma_0, \gamma_1, 2\gamma_2}
instead.

Note that, when using the sample mean and standard deviation of \mjseqn{x_f}
for \mjseqn{\mu_f} and \mjseqn{\sigma_f} respectively, and the optimal
parameters for \mjseqn{\gamma} estimated by MLE, we are actually calculating
the estimates \mjseqn{\hat{F}_0}.
}
}

\section{Bias and variance constants}{
The bias and variance constants \mjseqn{B_{kl}} and \mjseqn{V_{kl}} are
matrices of size \mjseqn{d \times d}. We will calculate their estimates.
\subsection{Plug-in estimator}{

Let \mjseqn{K} be the kernel function used (see pg. 15 of the supplement for
relevant conditions). Define the following definite integrals over the reals:
\mjsdeqn{I_B = \int_{\mathbb{R}} u^3 \dot K(u) du}
\mjsdeqn{I_V = \int_{\mathbb{R}} \dot K(u)^2 du}
Then the estimators are:
\mjsdeqn{\hat{B}_{kl} = I_B \frac{1}{n} \sum_{i=1}^n x_{r,k}^i x_{r,l}^i \left( \hat{F}^{1,3}(x_r^i) + \frac{1}{3} \hat{F}^{3,1}(x_r^i) \right)}
\mjsdeqn{\hat{V}_{kl} = 2 I_V \frac{1}{n} \sum_{i=1}^n \hat{F}^{0,1}(x_r^i) (x_{r,k}^i)^2 (x_{r,l}^i)^2}
where \mjseqn{x_{r,k}^i} is the \mjseqn{k}-th component of the \mjseqn{i}-th
observation of the vector \mjseqn{x_r}.

Note that we currently support the kernel function \mjseqn{K(u) = \phi(u)},
whose integrals are \mjseqn{I_B = -3} and \mjseqn{I_V = \frac{1}{4\sqrt{\pi}}}.
}

\subsection{Numerical differentiation estimator}{

The estimates are:
\mjsdeqn{\hat{B}_{kl} = - \frac{1}{n} \sum_{i=1}^n \left( (x_{r,k}^i)^3 x_{r,l}^i + x_{r,k}^i (x_{r,l}^i)^3 \right) \left( \hat{F}^{1,3}(x_r^i) + \frac{1}{3} \hat{F}^{3,1}(x_r^i) \right)}
\mjsdeqn{\hat{V}_{kl} = \frac{1}{8n} \sum_{i=1}^n \hat{F}^{0,1}(x_r^i) \left( 2|x_{r,k}^i| + 2|x_{r,l}^i| - |x_{r,k}^i + x_{r,l}^i| - |x_{r,k}^i - x_{r,l}^i|\right)}
}
}

\section{Step/bandwidth parameters}{
Let
\mjsdeqn{h_{kl} = \left( \frac{3}{4} \frac{\hat{V}_{kl}}{(\hat{B}_{kl})^2} \right)^{\frac{1}{7}} n^{-\frac{1}{7}}}
be the (matrix-valued) bandwidth of the plug-in method. The value of
\mjseqn{\epsilon_{kl}}, i.e. the step of the numerical differentiation method,
can be defined identically, but we also provide a scalar value, replacing
\mjseqn{\hat{V}_{kl}} and \mjseqn{\hat{B}_{kl}} with their sum over all
\mjseqn{k,l}.
}

\keyword{internal}
